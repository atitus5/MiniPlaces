\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{MiniPlaces Report}

\author{Andrew Titus (6.869), Brian Wheatman (6.869)\\
Massachusetts Institute of Technology\\
77 Massachusetts Avenue, Cambridge, MA 02139\\
{\tt\small \{atitus, wheatman\}@mit.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
The ability to determine what the surroundings are
in an image or video is important for various computer vision tasks,
such as motion planning and object recognition. Various
convolutional neural network architectures have shown great
promise in approaching human levels at this ability, called
scene recognition \cite{Places}. In this paper, we
investigate two such architectures, AlexNet \cite{AlexNet}
and VGG \cite{VGG}, and explore various modifications to these
networks in order to maximize performance on the MiniPlaces
dataset, a subset of the larger Places2 dataset \cite{Places}.
Results show that adjustments to hidden layer sizes, performing
batch normalization and using learning rate decay schedules with
early stopping yielded a $6.0\%$
relative improvement on test set Top-1 accuracy and a
$8.6\%$ relative improvement
on test set Top-5 accuracy when compared to the baselines of these
architectures.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Scene recognition is a classic task in computer vision.  Given an image, a system seeks to label the image with a class from some set of predefined scenes.  In recent years this has been done using convolutional neural networks\cite{AlexNet} \cite{VGG}.  We investigate several of these architectures to determine what are the necessary characteristics of effective scene recognition systems and how we might improve them.  

For this paper, we wish to be able to determine the scene category for the images
in MiniPlaces, a subset of the Places2 dataset \cite{Places}, out of 100
possible scenes. Our approach to this problem is to evaluate
various existing neural net architectures for computer vision and
seeing how they are able to do on the validation set of 10,000 images,
in order to determine areas of improvement to address with our own
modified models. We start by training baseline models based on
the AlexNet \cite{AlexNet} and VGG \cite{VGG} convolutional neural
network architectures as starting points and adapt the architecture
to our specific problem based on experimental results. These results
include Top-1 accuracy (percentage of examples for which the model
correctly guessed the true label) and Top-5 accuracy (percentage of
examples for which the true label was in the model's top 5 guesses),
as well as more practical considerations, such as training time,
memory usage and generalization to validation and evaluation datasets.

\subsection{Experimental Setup}
\label{exp_setup}
The MiniPlaces dataset consists of 120,000 images. Of these, 110,000 are labeled with one of 100 categories and 10,000 are not labeled.  We used these 100,000 of these labeled images as our training set and 10,000 of them as the validation set.  The 100,000 images in the training set had 1,000 of each category.  The 10,000 unlabeled images were the test set and were only used for submitting the results, which were then evaluated on the 6.869 course server using the ground truth labels not available to students in the course.

We had two different training setups. We used Amazon Web Services
with a \texttt{p2.xlarge} instance that had a NVIDIA K80 GPU with 61 GB memory and 4 virtual CPUs (vCPUs).  However, since we were limited in the amount of time we could use this for, we also used our local machine which had a NVIDIA GeForce GT 740M Graphics with 2048MB of memory. For both hardware stacks, we implemented our models in
Tensorflow with CuDNN GPU support using Python.

\subsection{Division of Work}

Authors shared equal workload in writing this report. However,
the research was divided as follows:

\subsubsection{Brian}
Brian focused on trying different ideas and seeing which seemed fruitful.  Brian started with the modifications to AlexNet as described in Section \ref{mod_alex}.  Then, he tried various approaches to tackle the problem of over-fitting, which are described in Sections \ref{ensemble} and \ref{mod_input}.

\subsection{Drew}
Drew focused on taking a deeper dive into VGG and working with AWS as described in Section \ref{vgg}.  This ended up being the most successful path and gave us our best results.

\section{Approaches}
%TODO review and edit
\subsection{Modification of AlexNet}
\label{mod_alex}
Our first approach was to try and modify AlexNet to see if we could gain any additional performance beyond its baseline model, renowned for its success in the 2012 ImageNet Challenge \cite{AlexNet}. We tried varying the sizes of the various layers, changing the Dropout rate and changing the sizes of the filters.  However, none of these resulted in any performance benefit.  Interestingly enough, they all had almost identical performance on the validation set, signifying that AlexNet is a fairly robust model to small changes in its structure.  The only change that we did that had any effect was adding a fully connected layer at the end.  This made the model a fair amount bigger and slower to train, but was able to yield a $2.4\%$ relative improvement in the Top-1 accuracy and $5.2\%$ relative improvement in the Top-5 accuracy on the evaluation set.  One thing that we noticed was that our model would start over-fitting fairly dramatically and changing the Dropout was not able to fix it, suggesting a need for better regularization techniques to be tried in the future.  These are labeled as ``AlexNet standard'' and ``AlexNet (extra FC)'' in Tables \ref{tab:top1_results} and \ref{tab:top5_results}.

\subsection{Ensemble methods}
\label{ensemble}
In an attempt to limit the amount of over-fitting, we tried to see if we could combine models that had been trained on different subsets of the images and lead to better generalization across different classes of scenes.  To do, we made several models, all with the same architecture, that would each get a different training set.  We would then train until the validation accuracy stopped improving.  When we had many such models, we would have each of them make predictions on the test/validation set and get the probability for each category that each model gave it.  We then sum these probabilities to get an overall probability for each scene category and can make our guesses.  The idea of this approach is that no individual model is required to generalize to all of the images (rather, each model is a form of ``expert''), but together they have seen all of the images.  We tested this by using AlexNet and training for 1,000 iterations on a random subset of 10,000 images  This approach, while not particularly successful at increasing the Top-5 accuracy, did show us something interesting: having multiple guesses at each image almost always increased the Top-1 accuracy over a single guess. This is discussed in more depth in
Section \ref{exp_results}.

\subsection{Modifying the input images}
\label{mod_input}
Following up on the above idea for which getting multiple guesses for each image and combining then, we tried to see if we could deal with over-fitting by training not just only on the raw input images, but also on transformations of them.  When loading an image instead of just having the one image, we would instead create and return 9 versions of this image.  These versions were combinations of three different sizes and three different positions of the raw image.  During the training phase, we considered all of these images separately, but during the validation and testing phase, we would combine the predictions for each of the images in the same way as we did above.  We did find that in general this would improve our training accuracy, but at a cost of making training substantially slower.  Since each image was now 9 separate images, and we found that it did best if these images were in the same batch, our batch size had to drop by a factor of 9, since we could not increase our memory by a factor of 9.  This meant that this model was too slow to be practically trained to any reasonable degree of convergence.

Another method we tried was taking one random image from this set of 9 in the training phase, then only using all 9 in the testing and validation validation phases. This made training more feasible, but was not able to generate improved results.  This is labeled as ``AlexNet Multi'' in Tables \ref{tab:top1_results} and \ref{tab:top5_results}.  We were not able to complete this with VGG due to the issues with the size of the model and the required batch size.

\subsection{VGG}
\label{vgg}

The Visual Geometry Group (VGG) at University of Oxford developed
several variations of the original 2012 AlexNet architecture
(see Section \ref{mod_alex}) for the 2014 ImageNet challenge
\cite{VGG}. Two main changes to the AlexNet architecture
were made for all such variations:
\begin{itemize}
\item Local Response Normalization (LRN) layers were removed, as
they observed increased memory consumption and computation time with
no increase in performance on the ImageNet dataset they were using.

\item Receptive fields were reduced in size from $11 \times 11$
with stride 4 to $3 \times 3$ with stride 1, with network depth
being increased to compensate.
\end{itemize}

In addition to these changes, there were several variations
on the network architectures themselves (see
Table \ref{tab:vgg_archs}). For brevity, we only
include the architectures upon which we conducted experiments
as well (VGG A, VGG B, VGG D), in addition to our own modifications
to these architectures (described later in
Section \ref{vgg_arch_mods}. As in the paper, the convolutional
layer parameters are denoted as
``conv(receptive field size)-(number of channels)'' and neither
Dropout layers nor the ReLU activation function are shown.

\begin{table}[th]
  \caption{VGG Architectures Used}
  \label{tab:vgg_archs}
  \centering
  \begin{tabular}{ c | c | c }
    \toprule
    VGG A & VGG B & VGG D \\
    \midrule
    11 weight layers & 13 weight layers & 16 weight layers \\
    \midrule
    \multicolumn{3}{c}{Input ($100 \times 100$ RGB image)} \\
    \midrule
    conv3-64 & conv3-64 & conv3-64 \\
     & conv3-64 & conv3-64 \\
    \midrule
    \multicolumn{3}{c}{maxpool} \\
    \midrule
    conv3-128 & conv3-128 & conv3-128 \\
     & conv3-128 & conv3-128 \\
    \midrule
    \multicolumn{3}{c}{maxpool} \\
    \midrule
    conv3-256 & conv3-256 & conv3-256 \\
    conv3-256 & conv3-256 & conv3-256 \\
     & & conv3-256 \\
    \midrule
    \multicolumn{3}{c}{maxpool} \\
    \midrule
    conv3-512 & conv3-512 & conv3-512 \\
    conv3-512 & conv3-512 & conv3-512 \\
     & & conv3-512 \\
    \midrule
    \multicolumn{3}{c}{maxpool} \\
    \midrule
    conv3-512 & conv3-512 & conv3-512 \\
    conv3-512 & conv3-512 & conv3-512 \\
     & & conv3-512 \\
    \midrule
    \multicolumn{3}{c}{maxpool} \\
    \midrule
    \multicolumn{3}{c}{Fully connected w/ 4096 output units} \\
    \midrule
    \multicolumn{3}{c}{Fully connected w/ 4096 output units} \\ 
    \midrule
    \multicolumn{3}{c}{Fully connected w/ 100 output classes} \\
    \midrule
    \multicolumn{3}{c}{Softmax} \\
    \midrule
    \bottomrule
  \end{tabular}
\end{table}

The baseline models from VGG were trained in the same way as
described in the paper. Mini-batch gradient descent with momentum
0.9, batch size 100, and initial learning rate $\gamma = 0.01$
was used to minimize our loss function, which was a sum of
the cross-entropy with an $\ell_2$ regularization penalty on
the weights of the network with $\ell_2$ weight
$\beta = 5 \times 10^{-4}$. The learning rate was then decayed
by a factor of 10 at the end of each epoch for which the
validation loss did not improve, and early stopping was employed
once the learning rate was decayed three times.

\subsubsection{Training modifications}
\label{vgg_train_mods}

Our primary modification to this training protocol was to 
introduce regularization for each batch, due to issues with
over-fitting in general as well as rapidly changing batch sizes
per model due to our memory constraints (see Section \ref{exp_setup}).
To address this, we added batch normalization \cite{BatchNorm} (abbreviated
as BN in Tables \ref{tab:top1_results} and \ref{tab:top5_results}).
Batch normalization had the effect of improving our generalization
by implicitly including normalization as part of the model
architecture, thus having the regularizing effect of reducing
internal covariate shift. Batch normalization led to $2.0\%$
absolute improvement in Top-5 error and $3.2\%$ absolute
improvement in Top-1 error on the evaluation set when applied
to the VGG A architecture and was also successfully applied to
VGG B and VGG D.

\subsubsection{Architecture modifications}
\label{vgg_arch_mods}

Modifications were also made to the VGG architectures themselves.
For the VGG A architecture, we experimented with adding an extra
fully-connected layer with 4096 output units (labeled as
``VGG A (extra FC) w/ BN'' in Table \ref{tab:top1_results} and
\ref{tab:top5_results}) after observing improved results for
doing so with AlexNet (see the aforementioned tables),
but this seemed to only have the effect of increasing under-fitting
in our network. Thus, we also experimented with removing layers.

The ``VGG A (small) w/ BN'' network is a modification to VGG A where
the last two convolutional layers (and associated maxpool) were
removed. This seemed to perform better than adding layers and took
less time to train, but was ultimately less effective than the
original VGG A architecture with batch normalization applied.
These architecture modifications were not applied to VGG B
or VGG D, as these already were exhibiting under-fitting relative
to VGG A models.


\subsection{Model combination}
Finally, we tried combining the AlexNet and VGG guesses in a similar
manner to the averaging performed by the ensemble models described
in Section \ref{ensemble}. While this performed better than the
baseline AlexNet and VGG models in terms of Top-1 accuracy, it did
not outperform AlexNet or VGG on Top-5 accuracy (similar to how
ensemble models performed relative to their individual classifiers).


%------------------------------------------------------------------------
\section{Experimental Results}
\label{exp_results}
%embellish
Our data is shown in Tables \ref{tab:top1_results} and \ref{tab:top5_results}.  This includes all of our submitted results, except those that were from buggy models.  We note several interesting points.  The first is that VGG always performs better than AlexNet.  We also note that ensemble methods do much better in improving Top-1 error compared to Top-5 error.  We hypothesize that this is because of the relatively na\"{i}ve way in which the models were combined.  Our averaging choice had the effect of allowing one of the models to guess correctly for the whole ensemble if it was fairly certain about what the result was.  However, in the case where the models are less certain and instead have the correct answer in just the Top-5 guesses, it is likely to be overtaken by a different label when each of the models average together their (potentially disjoint)
guesses for Top-5 labels.

\begin{table}[th]
  \caption{Top-1 Error on MiniPlaces dataset}
  \label{tab:top1_results}
  \centering
  \begin{tabular}{ l | r | r }
    \toprule
    Model & Validation & Evaluation \\
    \midrule
    % VGG A (overfitted) & $64.0\%$ & $74.5\%$ \\
    AlexNet Multi & $55.2\%$ & $54.4\%$ \\
    AlexNet + VGG A & $51.9\%$ & $52.8\%$ \\
    AlexNet (extra FC) 10,000 iterations  & $64.5\%$ & $65.3\%$ \\
    AlexNet standard 10,000 iterations & $61.4\%$ & $62.9\%$ \\
    AlexNet (extra FC) 25,000 iterations & $61.2\%$ & $61.4\%$ \\
    VGG A w/o LR decay & $63.0\%$ & $55.7\%$ \\
    VGG D w/ BN & $52.3\%$ & $52.9\%$ \\
    VGG A & $55.0\%$ & $52.9\%$ \\
    VGG A (extra FC) w/ BN & $50.6\%$ & $51.1\%$ \\
    VGG A (small) w/ BN & $49.7\%$ & $50.4\%$ \\
    VGG B w/ BN & $49.6\%$ & $50.5\%$ \\
    VGG A w/ BN & $\mathbf{48.8\%}$ & $\mathbf{49.7\%}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[th]
  \caption{Top-5 Error on MiniPlaces dataset}
  \label{tab:top5_results}
  \centering
  \begin{tabular}{ l | r | r }
    \toprule
    Model & Validation & Evaluation \\
    \midrule
    % VGG A (overfitted) & $37.0\%$ & $45.9\%$ \\
    AlexNet Multi & $24.6\%$ & $40.9\%$ \\
    AlexNet + VGG A & $21.7\%$ & $39.3\%$ \\
    AlexNet (extra FC) 10,000 iterations & $34.2\%$ & $34.3\%$ \\
    AlexNet standard 10,000 iterations & $31.1\%$ & $32.5\%$ \\
    AlexNet (extra FC) 25,000 iterations & $30.5\%$ & $30.9\%$ \\
    VGG A w/o LR decay & $28.0\%$ & $25.8\%$ \\
    VGG D w/ BN & $22.3\%$ & $23.4\%$ \\
    VGG A & $17.0\%$ & $23.3\%$ \\
    VGG A (extra FC) w/ BN & $20.6\%$ & $22.0\%$ \\
    VGG A (small) w/ BN & $21.2\%$ & $21.6\%$ \\
    VGG B w/ BN & $\mathbf{20.2\%}$ & $21.3\%$ \\
    VGG A w/ BN & $20.4\%$ & $\mathbf{21.3\%}$ \\
    \bottomrule
  \end{tabular}
\end{table}


\section{Conclusion}
We found that both networks that we tested are reasonably robust to small changes in the network structure, but did find several techniques that could be used to improve the performance on this specific task at different costs.  Adding a fully connected layer to the end of a network normally improved the performance, but required more iterations to learn.  We also found that combining models could improve the accuracy on Top-1, but not Top-5 accuracy.  We also found that taking different sections of the image dataset and making individual guesses on each one could also improve performance, but required a multiplicative increase in memory. 
We conclude that both AlexNet and VGG are robust, effective architectures
for various computer vision tasks that are capable of being tuned to
other tasks and other datasets.


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}